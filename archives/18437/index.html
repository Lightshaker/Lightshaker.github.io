<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lightshaker.cn","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ApacheCN社区 Hadoop2.x与3.x端口变化 Maven Repository Spark与Scala版本对应关系&#x2F;Maven中央库  本文Spark配置基于通过VirtualBox搭建的Hadoop集群（3台Ubuntu18.04-Server服务器）环境； 涉及到的包版本：  JDK1.8 jdk-8u271-linux-x64.tar.gz hadoop-3.3.0 hadoo">
<meta property="og:type" content="article">
<meta property="og:title" content="Ubuntu18.04搭建Hadoop集群环境并配置Spark3.0">
<meta property="og:url" content="https://lightshaker.cn/archives/18437/index.html">
<meta property="og:site_name" content="Journey">
<meta property="og:description" content="ApacheCN社区 Hadoop2.x与3.x端口变化 Maven Repository Spark与Scala版本对应关系&#x2F;Maven中央库  本文Spark配置基于通过VirtualBox搭建的Hadoop集群（3台Ubuntu18.04-Server服务器）环境； 涉及到的包版本：  JDK1.8 jdk-8u271-linux-x64.tar.gz hadoop-3.3.0 hadoo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/1.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/2.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/3.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/4.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/5.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/6.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/7.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/8.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/17.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/18.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/9.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/10.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/11.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/12.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/23.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/24.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/25.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/26.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/27.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/28.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/29.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/30.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/31.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/33.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/34.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/43.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/44.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/45.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/46.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/13.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/14.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/15.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/16.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/19.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/20.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/21.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/22.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/32.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/35.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/36.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/37.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/38.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/39.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/40.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/41.png">
<meta property="og:image" content="https://lightshaker.cn/archives/18437/42.png">
<meta property="article:published_time" content="2021-01-23T06:07:40.000Z">
<meta property="article:modified_time" content="2023-06-12T06:53:01.976Z">
<meta property="article:author" content="Lightshaker">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lightshaker.cn/archives/18437/1.png">

<link rel="canonical" href="https://lightshaker.cn/archives/18437/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Ubuntu18.04搭建Hadoop集群环境并配置Spark3.0 | Journey</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Journey</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lightshaker.cn/archives/18437/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lightshaker">
      <meta itemprop="description" content="It is our choices that show what we truly are, far more than our abilities.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Journey">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Ubuntu18.04搭建Hadoop集群环境并配置Spark3.0
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-01-23 14:07:40" itemprop="dateCreated datePublished" datetime="2021-01-23T14:07:40+08:00">2021-01-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-12 14:53:01" itemprop="dateModified" datetime="2023-06-12T14:53:01+08:00">2023-06-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span id="/archives/18437/" class="post-meta-item leancloud_visitors" data-flag-title="Ubuntu18.04搭建Hadoop集群环境并配置Spark3.0" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/18437/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/18437/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/apachecn">ApacheCN社区</a></p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/qq_31454379/article/details/105439752">Hadoop2.x与3.x端口变化</a></p>
<p><a target="_blank" rel="noopener" href="https://mvnrepository.com/">Maven Repository</a></p>
<p><a
target="_blank" rel="noopener" href="https://mvnrepository.com/artifact/org.apache.spark/spark-sql?__cf_chl_captcha_tk__=0a05a22d352bf3c4cbd0c47a74bfed21a5a5a0c1-1611893236-0-AXkT62L0gxD-4zgaJ_Zrg2rFBsJqcpfKKLCPw8k4uYMXO08bntOJITLYBwQ8M8TVlBUox8OsRS8ZXiH6v37KMAScpua055_Yu9ZsmCaJ8VHAHkS6p-B5Llr8n-GOyh6RWwrbSHaTl3JRGMijzrtLoo82W0D0d5jvrvLaMD4aS2HEmbj01KPefdIMG20BYi_KgsaYpLsXfNZql0jXd-bobaOFhnBaVxVq7pxRV2AZyeS2au4i-xLl5NfJoYQZu3b4pwqLUyy690_Zl60Ui1C4q634mGbx_L3kn12o3nU_kvFJTFrO1_T8Iscor2yO9ugzg-jnAVUnLUPjuM2OJ7P8-kW11rYmwQ_km5PRcQ96V5aetKHq-ttnbFuYk0MYVn1TmHY5Dcp5C6X4PSZKpJEu_No2XLtZDE9ryeoReBfKa1qpcZDiHxe63iWnaTiq2zFz7t0OTs98IBGqwkBRjkuwZjRm9pOj3j47KuwRKB6_caQ60QZx5XV9f1XCNvHHU_bmU8Xar4X5cI2mJVzQzKhkbDMlAtYh6XhhGF4nnrZ07Rhzu8aT_yr7IgA_79AKwijrg9egZIR0Wj7eHrEzYZbcbeUzT-zkyi2nOwpIRHsldZFJop-y_hUIkgExuPkhUsDJ6A">Spark与Scala版本对应关系/Maven中央库</a></p>
</blockquote>
<p>本文Spark配置基于通过<strong>VirtualBox</strong>搭建的Hadoop集群（3台Ubuntu18.04-Server服务器）环境；</p>
<p>涉及到的包版本：</p>
<ul>
<li><p>JDK1.8 <code>jdk-8u271-linux-x64.tar.gz</code></p></li>
<li><p>hadoop-3.3.0 <code>hadoop-3.3.0.tar.gz</code></p></li>
<li><p>spark-3.0.1 <code>spark-3.0.1-bin-hadoop3.2.tgz</code></p></li>
<li><p>scala-2.12.13 <code>scala-2.12.13.tgz</code></p></li>
<li><p>hbase-2.4.0 <code>hbase-2.4.0-bin.tar.gz</code></p></li>
<li><p>zookeeper-3.6.2
<code>apache-zookeeper-3.6.2-bin.tar.gz</code></p></li>
</ul>
<span id="more"></span>
<h2
id="使用virtualbox搭建基于ubuntu-server的三机hadoop集群">使用VirtualBox搭建基于Ubuntu-Server的三机Hadoop集群</h2>
<h3 id="集群规划">集群规划</h3>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 90%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">主机名</th>
<th style="text-align: center;">功能 hadoop|spark</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">master</td>
<td style="text-align: center;">NameNode | Master, Zookeeper</td>
</tr>
<tr class="even">
<td style="text-align: center;">slave1</td>
<td style="text-align: center;">datanode, subNameNode,
mapreduce.jobhistory | Master, Zookeeper, Worker</td>
</tr>
<tr class="odd">
<td style="text-align: center;">slave2</td>
<td style="text-align: center;">datanode, yarn.resourceManager |
ZooKeeper, Worker</td>
</tr>
</tbody>
</table>
<h3 id="创建master虚拟机并配置环境">创建Master虚拟机并配置环境</h3>
<h4 id="创建虚拟机">创建虚拟机</h4>
<p>使用VirtualBox创建名为Master.Hadoop的虚拟机</p>
<p>点击VBox管理器界面的 <code>全局工具</code> -&gt;
<code>主机网络管理器</code>，若下方内容为空，则创建一个新的（默认名称为vboxnet0，注意创建完点击<code>启用</code>），然后返回主界面，右键设置Master.Hadoop，在
<code>网络</code>
一栏中默认存在网卡1，连接方式为网络地址转换（NAT），我们启用网卡2，设置连接方式为
<code>仅主机(Host-Only)</code>，界面名称即为vboxnet0（或手动分配的名称）</p>
<p><img src="1.png" style=zoom:100%></p>
<p>启动Master.Hadoop虚拟机，载入
<code>ubuntu-18.04.5-live-server-amd64.iso</code>
镜像文件，开始安装，在进行网络设置的时候将自动分配192.168.56.xxx地址的网卡设为手动（manual）,
将子网subnet设置为<code>192.168.56.0/24</code>,
将地址Address设置为<code>192.168.56.104</code>（可任意，只要不与已分配过的重复即可）</p>
<p><img src="2.png" style=zoom:100%></p>
<p><img src="3.png" style=zoom:100%></p>
<p><img src="4.png" style=zoom:100%></p>
<p>一直继续到设置主机名和用户名的界面</p>
<p>设置主机名为master,
用户名为node，以及设置用户密码，一路点done完成安装，重启登录；</p>
<p><img src="5.png" style=zoom:100%></p>
<p><img src="6.png" style=zoom:100%></p>
<h4 id="安装sshjdk与hadoop">安装SSH、JDK与Hadoop</h4>
<h5 id="安装并配置ssh">安装并配置SSH</h5>
<blockquote>
<p>所谓"公钥登录"，原理很简单，就是用户将自己的公钥储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回来。远程主机用事先储存的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求密码。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install openssh-server</span><br><span class="line">ssh localhost <span class="comment"># 此处为了创建一个.ssh目录，肯定是登录不了的</span></span><br><span class="line"><span class="built_in">cd</span> ~/.ssh/</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"><span class="built_in">cat</span> ./id_rsa.pub &gt;&gt; ./authorized_keys <span class="comment"># 将公钥设置为服务端授权公钥</span></span><br></pre></td></tr></table></figure>
<p>授权公钥存放于服务器端，因此在一台存有匹配私钥 <code>id_rsa</code>
的客户机上远程登录服务器可以实现免密，因此现在服务器登录本机localhost是免密的；</p>
<p>要使从这台机器ssh远程登录另一台机器免密，只需要把已授权的公钥文件
<code>id_rsa.pub</code> 发送给那台机器，保存为
<code>.ssh/authorized_keys</code>
即可（如果已存在该文件，就追加到文末）</p>
<p>由此可知，要使三台Hadoop服务器实现两两免密登录，只需要将同一份配置好的
<code>.ssh</code>
文件夹复制到三台机器的用户路径下，就能让每台机子都拥有同一份服务端授权公钥
<code>authorized_keys</code> 以及客户端私钥
<code>id_rsa</code>；而我们创建集群就是通过先配置好一台机器，然后进行整体复制，再修改副本的一些细枝末节（例如主机名，ip地址等）实现的，在复制的过程中自然就配置好了ssh两两免密；</p>
<p>注意，免密登录的账户是与授权公钥相匹配的私钥所有者账户，由于默认
<code>.ssh</code> 文件夹存放于 <code>/home/user/</code>
路径下，因此免密登录只能登录 user
账户，本例中user是用户node，如果ssh登录的是root或其他账户，同样需要输入密码；可以<strong>在完成以下所有配置后</strong>从终端测试一下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hadoop.Master服务器端</span></span><br><span class="line">ssh node@192.168.56.104 <span class="comment"># 自身</span></span><br><span class="line">ssh node@192.168.56.105 <span class="comment"># slave1</span></span><br><span class="line">ssh node@192.168.56.106 <span class="comment"># slave2</span></span><br><span class="line"><span class="comment"># 用root账户登录三台机器都需要密码</span></span><br><span class="line">ssh node@192.168.56.104</span><br></pre></td></tr></table></figure>
<h5 id="安装jdk与hadoop">安装JDK与Hadoop</h5>
<p>主机端进入<a
target="_blank" rel="noopener" href="https://www.oracle.com/java/technologies/javase-downloads.html">JAVA官网</a>，点击Java
SE 8栏目中的 <code>JDK Download</code>，下载jdk1.8压缩包
<code>jdk-8u271-linux-x64.tar.gz</code></p>
<p>主机端进入<a
target="_blank" rel="noopener" href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz">Hadoop官网</a>，下载hadoop的binary压缩包
<code>hadoop-3.3.0.tar.gz</code></p>
<p>配置主机与ubuntu-server虚拟机的共享文件夹，详见<a
target="_blank" rel="noopener" href="https://www.lightshaker.cn/2021/01/19/vbox-share/">主机ubuntu与Vbox虚拟机ubuntu-server实现文件共享</a></p>
<p>通过共享文件夹将JDK和Hadoop压缩包传给虚拟机</p>
<p>在虚拟机端分别解压JDK和Hadoop到 <code>/usr/local/</code> 路径下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf jdk-8u271-linux-x64.tar.gz -C /usr/local/jdk1.8 <span class="comment"># 重命名文件夹</span></span><br><span class="line">sudo tar -zxvf hadoop-3.3.0.tar.gz -C /usr/local/</span><br></pre></td></tr></table></figure>
<p>配置统环境变量　<code>sudo vim /etc/profile</code>，添加</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/local/jdk1.8</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span>/jre</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop-3.3.0</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib:<span class="variable">$&#123;JRE_HOME&#125;</span>/lib</span><br><span class="line"><span class="built_in">export</span> PATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin:<span class="variable">$&#123;HADOOP_HOME&#125;</span>/sbin:<span class="variable">$&#123;PATH&#125;</span></span><br></pre></td></tr></table></figure>
<p>将jdk和hadoop的文件夹所有权交给node用户</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">chown</span> -R node /usr/local/jdk1.8</span><br><span class="line">sudo <span class="built_in">chown</span> -R node /usr/local/hadoop-3.3.0</span><br></pre></td></tr></table></figure>
<p>重启虚拟机，或在终端里 <code>source /etc/profile</code></p>
<p>查看是否安装成功</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure>
<p><img src="7.png" style=zoom:100%></p>
<h4 id="配置hadoop">配置Hadoop</h4>
<p>由于直接在虚拟机里编辑文件内容较麻烦，可以采取两种方案让编辑文件更方便：</p>
<ul>
<li><p>方案一：通过虚拟机与主机的共享文件夹将待编辑文件传输到主机中，编辑完毕再传回去；</p></li>
<li><p>方案二：使用VScode或Sublime插件，通过ssh连接到服务器，直接编辑文件（推荐）；</p></li>
</ul>
<p>本文采用的是方案二，通过VScode插件Remote-SSH连接虚拟机，可以通过共享文件夹将主机端生成的授权公钥复制到虚拟机端
<code>/root/.ssh/authorized_keys</code>，实现免密；</p>
<ul>
<li><p>注意VScode需要登录虚拟机服务器端的root账户才能编辑
<code>/usr/local</code> 路径下的文件，因此需要将授权公钥存在
<code>/root</code> root用户路径下；</p></li>
<li><p>配置过程与报错解决方案见文末 “问题与解决方案” 问题三；</p></li>
</ul>
<p><img src="8.png" style=zoom:100%></p>
<p>Hadoop配置文件路径为：<code>/usr/local/hadoop-3.3.0/etc/hadoop/</code>；</p>
<p>以下xml与shell配置内容均包含在
<code>&lt;configuration&gt; &lt;/configuration&gt;</code> 块内；</p>
<h5 id="core-site.xml">core-site.xml</h5>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop-3.3.0/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="hdfs-site.xml">hdfs-site.xml</h5>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 数据的副本数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助namenode主机 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="yarn-site.xml">yarn-site.xml</h5>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="mapred-site.xml">mapred-site.xml</h5>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br></pre></td></tr></table></figure>
<h5 id="hadoop-env.sh-与-yarn-env.sh">hadoop-env.sh 与 yarn-env.sh</h5>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/local/jdk1.8</span><br></pre></td></tr></table></figure>
<h5 id="workers">workers</h5>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
<h5 id="修改hosts文件">修改hosts文件</h5>
<p><code>sudo vim /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.56.104	master</span><br><span class="line">192.168.56.105	slave1</span><br><span class="line">192.168.56.106	slave2</span><br></pre></td></tr></table></figure>
<h3
id="通过master副本创建slave虚拟机">通过Master副本创建Slave虚拟机</h3>
<blockquote>
<p>如果需要配置Spark的话，最好在Master上配置完Spark再回到这里创建副本，否则之后每台机子都得配置一次，很麻烦</p>
</blockquote>
<ul>
<li><p>打开VBox管理器，右键虚拟机
<code>Master.Hadoop</code>，选择复制，副本命名为
<code>Slave1.Hadoop</code></p></li>
<li><p>勾选 <code>完全复制并初始化所有网卡mac</code>，下一步选择
<code>完全复制</code>；</p></li>
<li><p>启动并登录 <code>Slave1.Hadoop</code>, 编辑主机名
<code>sudo vim /ect/hostname</code>，更改为 <code>slave1</code></p></li>
<li><p>编辑网络配置文件
<code>vim /etc/netplan/00-installer-config.yaml</code></p></li>
<li><p>将 <code>address</code> 一项的 ip 地址修改为
<code>192.168.56.105/24</code>；</p></li>
<li><p>以同样的复制 <code>Slave2.Hadoop</code>，修改主机名为
<code>slave2</code>，ip为 <code>192.168.56.106/24</code>；</p></li>
</ul>
<h3 id="启动与关闭hadoop">启动与关闭Hadoop</h3>
<h4 id="启动hadoop">启动Hadoop</h4>
<p>启动三台虚拟机
<code>Master.Hadoop</code>，<code>Slave1.Hadoop</code>，<code>Slave2.Hadoop</code></p>
<ul>
<li>在<strong>master</strong>端初始化文件系统并启动dfs；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>
<ul>
<li>在部署为<code>yarn.resourcemanager</code>的<strong>slave2</strong>端启动yarn；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>
<ul>
<li>在部署为<code>mapreduce.jobhistory</code>的<strong>slave1</strong>端启动historyserver；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>
<h4 id="通过ip访问hadoop集群">通过ip访问Hadoop集群</h4>
<p><strong>hdfs集群：192.168.56.104:9870</strong></p>
<ul>
<li><p>NameNode：负责管理文件系统名称空间和控制外部客户机访问的主机</p></li>
<li><p>本例中NameNode是<code>master</code>，Hadoop3默认端口为9870；</p></li>
</ul>
<p><strong>yarn集群：192.168.235.131:8088</strong></p>
<ul>
<li><p>ResourceManager：yarn集群的主控节点，负责协调和管理整个集群（NodeManager）的资源。</p></li>
<li><p>本例中RecourceManager是<code>slave2</code>，Hadoop3默认端口为8088</p></li>
</ul>
<p><img src="17.png" style=zoom:100%></p>
<p><img src="18.png" style=zoom:100%></p>
<h4 id="关闭hadoop">关闭Hadoop</h4>
<p>可以执行 <code>stop-all.sh</code> 关闭所有节点；</p>
<ul>
<li><strong>master</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>slave2</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-yarn.sh</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>slave1</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure>
<h2 id="配置spark3.0环境">配置Spark3.0环境</h2>
<h3 id="下载spark压缩包">下载Spark压缩包</h3>
<p>本地主机端进入<a
target="_blank" rel="noopener" href="http://spark.apache.org/downloads.html">Spark官网</a>，下载对应Hadoop版本的Spark3压缩包。由于上文配置的Hadoop版本为3.3.0，因此
<code>package</code> 选择支持Hadoop3.2以上的版本
<code>Pre-built for Apache Hadoop 3.2 and later</code>，点击下方的
<code>spark-3.0.1-bin-hadoop</code>；</p>
<p><img src="9.png" style=zoom:100%></p>
<p>通过共享文件夹 <code>VMshare</code>，将压缩包复制到虚拟机中，并解压到
<code>/usr/local/</code> 路径下，且重命名文件夹为
<code>spark-3.0.1</code>；</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf ./spark-3.0.1-bin-hadoop.tgz -C /usr/local</span><br><span class="line">sudo <span class="built_in">mv</span> /usr/local/spark-3.0.1-bin-hadoop /usr/local/spark-3.0.1</span><br></pre></td></tr></table></figure>
<p>使用 <code>ls -ll | grep spark</code>
命令，确保spark文件夹的拥有者是node用户；</p>
<p><img src="10.png" style=zoom:100%></p>
<h3 id="配置spark-env.sh">配置spark-env.sh</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark-3.0.1/conf</span><br><span class="line">sudo <span class="built_in">cp</span> ./spark-env.sh.template ./spark-env.sh</span><br></pre></td></tr></table></figure>
<p><code>sudo vim ./spark-env.sh</code>，文末添加</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/local/java/jdk1.8</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=master</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077 <span class="comment"># 指定Spark中Master的内部访问端口，外部端口(Web)默认是8080</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-3.3.0/bin/hadoop classpath)</span><br></pre></td></tr></table></figure>
<p>其中 <code>SPARK_DIST_CLASSPATH</code>
的路径取决于你指定的hadoop可执行文件安装路径；</p>
<h3 id="配置slaves文件">配置slaves文件</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark-3.0.1/conf</span><br><span class="line">sudo <span class="built_in">cp</span> ./slaves.template ./slaves</span><br></pre></td></tr></table></figure>
<p><code>sudo vim ./slaves</code>，将文末默认存在的localhost注释掉，写入两个slave主机名，作为worker</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
<h3 id="配置环境变量">配置环境变量</h3>
<p><code>sudo vim /etc/profile</code>，将Spark路径下的 <code>bin</code>
和 <code>sbin</code> 路径添加到 <code>PATH</code> 变量中；</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/local/spark-3.0.1</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;PATH&#125;</span>:<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin:<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin</span><br></pre></td></tr></table></figure>
<p>重启电脑或直接
<code>source /etc/profile</code>，反正登录虚拟机也只用这一个shell；</p>
<h3 id="验证本地spark">验证本地Spark</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run-example SparkPi 2&gt;&amp;1 | grep <span class="string">&quot;Pi is&quot;</span> <span class="comment"># 2&gt;&amp;1 将日志信息都输出到stdout中，而不是输出到终端（输出日志的特殊性）</span></span><br></pre></td></tr></table></figure>
<p><img src="11.png" style=zoom:100%></p>
<h3 id="使用spark-shell">使用Spark Shell</h3>
<p>打开spark-shell，自动加载scala交互式命令行，自动创建名为
<code>sc</code> 的 <code>spark context</code> 对象，和名为
<code>sqlContext</code> 的 <code>sql context</code> 对象；</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure>
<p>打印 <code>Hello World</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(&quot;Hello World&quot;)</span><br></pre></td></tr></table></figure>
<p>在spark-shell中测试载入本地文件：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile = sc.textFile（<span class="string">&quot;file:///usr/local/spark-3.0.1/README.md&quot;</span>） <span class="comment">// 载入文件内容</span></span><br><span class="line">textFile.first <span class="comment">// 输出文件内容的第一行</span></span><br></pre></td></tr></table></figure>
<p><img src="12.png" style=zoom:100%></p>
<p>tips：交互式scala环境中清屏的方式为 <code>ctrl+l</code></p>
<h3 id="验证分布式spark">验证分布式Spark</h3>
<p>确认三台虚拟机均为启动状态，确保dfs和spark均启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># master</span></span><br><span class="line">start-dfs.sh</span><br><span class="line">/usr/local/spark-3.0.1/sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>提交SparkPi应用到Spark集群主机端执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark-3.0.1</span><br><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 ./examples/jars/spark-examples_2.12-3.0.1.jar 10</span><br></pre></td></tr></table></figure>
<ul>
<li><code>--class</code> 表示待执行应用的主类</li>
<li><code>--master spark://master:7077</code>
指定"standalone"独立部署模式连接到Spark集群</li>
<li><code>spark-examples_2.12-3.0.1.jar</code>
指定应用的运行类所在的jar包路径</li>
<li><code>10</code> 是程序的入口参数，用于设定当前应用的任务数量</li>
</ul>
<p>执行 <code>spar-submit</code>
提交应用的日志会保存在作为worker的slave1和slave2的
<code>/usr/local/spark-3.0.1/work</code>
路径下，需要定期删除一次，不然长期下来容易导致磁盘空间不足；</p>
<p>如图所示，表示配置成功</p>
<p><img src="23.png" style=zoom:100%></p>
<p>打开浏览器，通过Spark默认的Master外部端口
<code>master:8080</code>，访问Spark集群</p>
<p><img src="24.png" style=zoom:100%></p>
<h3 id="配置spark历史服务">配置Spark历史服务</h3>
<p>因为spark端口 <code>master:4040</code>
只有在spark-shell运行时才开放访问，为了在shell关闭时也能查看历史任务，需要配置历史服务器记录任务的运行情况；</p>
<p>为了避免一些报错，先在master中关闭hadoop和spark集群</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/hadoop-3.3.0/sbin/stop-all.sh</span><br><span class="line">/usr/local/spark-3.0.1/sbin/stop-all.sh</span><br></pre></td></tr></table></figure>
<p>在master端<strong>启动hdfs</strong>，新建文件夹，命名为
<code>logs</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Master端</span></span><br><span class="line">start-dfs.sh</span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> /logs</span><br></pre></td></tr></table></figure>
<p>通过asbru-cm等工具连接三台虚拟机，同时进行以下配置操作（或配置好一台后通过共享文件夹或ssh文件分发的方式传给另外两台）</p>
<p>进入spark路径下的 <code>conf</code> 文件夹，修改
<code>spark-defaults.conf</code> 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark-3.0.1/conf</span><br><span class="line">sudo <span class="built_in">cp</span> ./spark-defaults.conf.template ./spark-defaults.conf</span><br><span class="line">sudo vim ./spark-defaults.conf</span><br></pre></td></tr></table></figure>
<p>写入以下内容，注意9000为配置hadoop时在 <code>core-site.xml</code>
内指定的 NN RPC
端口，如果没设置，hadoop3默认端口为8020；（必须启动hadoop集群，且确保dir目录存在）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled             true</span><br><span class="line">spark.eventLog.dir                 hdfs://master:9000/logs</span><br></pre></td></tr></table></figure>
<p>修改 <code>conf/spark-env.sh</code> 文件，添加内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span>  SPARK_HISTORY_OPTS=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080 # 历史服务器 Web UI 端口</span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://master:9000/logs # 日志存放位置</span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30&quot;</span></span><br></pre></td></tr></table></figure>
<p>其中最后一项数字 <code>30</code> 表示保存 Application
历史记录的个数，超过该数量时旧的将被覆盖。这个是内存中的应用数，不是Web页面上显示的应用数。</p>
<p>配置完成后重启spark集群和历史服务器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark-3.0.1</span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>
<p>提交SparkPi测试应用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 ./examples/jars/spark-examples_2.12-3.0.1.jar 10</span><br></pre></td></tr></table></figure>
<p><img src="25.png" style=zoom:100%></p>
<p>此时在 <code>hdfs://master:9000/logs</code>
路径下会生成一个日志文件</p>
<p><img src="26.png" style=zoom:100%></p>
<p>可通过浏览器访问 <code>master:18080</code>，查看历史任务记录</p>
<p><img src="27.png" style=zoom:100%></p>
<h2 id="可选部分">可选部分</h2>
<h3
id="配置zookeeper与spark高可用集群">配置Zookeeper与Spark高可用集群</h3>
<p>按照上述配置完成后已经能使用了，但由于集群中Master节点只有一个，存在单点故障问题，因此在对安全性要求高的情况下有必要配置多个Master节点。</p>
<p>当处于活动状态的Master发生故障时，启用备用的Master，确保作业可以正常执行下去。一般采用Zookeeper配置高可用。</p>
<p>主机进入<a
target="_blank" rel="noopener" href="https://zookeeper.apache.org/releases.html">Zookeeper官网</a>，下载最新版压缩包，当前时间点最新的是
<code>apache-zookeeper-3.6.2-bin.tar.gz</code></p>
<p>通过共享文件夹将压缩包传输给三台虚拟机，并解压到
<code>/usr/local</code> 路径下，对文件夹作重命名</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /mnt/share</span><br><span class="line">sudo tar -zxvf ./apache-zookeeper-3.6.2-bin.tar.gz -C /usr/local</span><br><span class="line"><span class="built_in">cd</span> /usr/local</span><br><span class="line">sudo <span class="built_in">mv</span> ./apache-zookeeper-3.6.2-bin ./zookeeper-3.6.2</span><br></pre></td></tr></table></figure>
<p>配置环境变量 <code>sudo vim /etc/profile</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/usr/local/zookeeper-3.6.2</span><br><span class="line"><span class="built_in">export</span> PATH = <span class="variable">$&#123;PATH&#125;</span>:<span class="variable">$&#123;ZOOKEEPER_HOME&#125;</span>/bin</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<p>编辑 <code>zoo.cfg</code> 配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/zookeeper-3.6.2/conf</span><br><span class="line">sudo <span class="built_in">cp</span> ./zoo_sample.cfg ./zoo.cfg</span><br><span class="line">sudo vim ./zoo.cfg</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改dataDir</span></span><br><span class="line">dataDir=/usr/local/zookeeper-3.6.2/data</span><br><span class="line"><span class="comment"># 添加server</span></span><br><span class="line">server.0=master:2888:3888</span><br><span class="line">server.1=slave1:2888:3888</span><br><span class="line">server.2=slave2:2888:3888</span><br></pre></td></tr></table></figure>
<p>接着在<strong>三台</strong>虚拟机的 <code>dataDir</code>
路径中创建文件 <code>myid</code>，文件内容为 <code>server.x</code>
的数字 <code>x</code></p>
<p>例如：在master主机中创建文件
<code>/usr/local/zookeeper-3.6.2/data/myid</code>，文件内容为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure>
<p>其他两台按照同样的方式配置；</p>
<p>然后编辑 <code>spark-env.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /usr/local/spark-3.0.1/conf/spark-env.sh</span><br></pre></td></tr></table></figure>
<p>注释掉之前指定的 <code>SPARK_MASTER_HOST</code> 以及
<code>SPARK_MASTER_PORT</code>，添加
<code>SPARK_DAEMON_JAVA_OPTS</code>，之前历史服务器的配置可以保留。</p>
<p>以下为完整的配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/local/java/jdk1.8</span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-3.3.0/bin/hadoop classpath)</span><br><span class="line"><span class="comment">#export SPARK_MASTER_HOST=master</span></span><br><span class="line"><span class="comment">#export SPARK_MASTER_PORT=7077</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER </span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 </span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/spark&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080</span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://master:9000/logs</span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30&quot;</span></span><br></pre></td></tr></table></figure>
<p>其中修改 <code>SPARK_MASTER_WEBUI_PORT</code>
为8989是为了避免zookeeper工作时可能占用8080端口导致冲突；</p>
<p>全部配置完成后，首先在<strong>每台</strong>虚拟机中启动zookeeper</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure>
<p>然后在<strong>master</strong>中启动spark集群</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark-3.0.1</span><br><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>在slave1中启动备用master</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/spark-3.0.1</span><br><span class="line">sbin/start-master.sh</span><br></pre></td></tr></table></figure>
<p>现在打开浏览器，可以分别访问 <code>master:8989</code> 和
<code>slave1:8989</code>，注意二者的区别在于master的status显示为<strong>ALIVE</strong>，表示活跃状态，而slave1显示<strong>STANDBY</strong>，表示待激活状态，当master宕机后，slave1作为备用master就会变成ALIVE状态工作；</p>
<p><img src="28.png" alt="master:8989,ALIVE"></p>
<p><img src="29.png" alt="slave1:8989,STANDBY"></p>
<p>当以高可用方式运行Spark集群时，提交应用需要提交到所有master上，例如</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077,slave1:7077 ./examples/jars/spark-examples_2.12-3.0.1.jar 10</span><br></pre></td></tr></table></figure>
<p>最后测试一下将master进程杀死后，slave1是否会通过zookeeper判断宕机状态，并及时启动接管master工作；</p>
<p>在master端通过 <code>jps</code> 命令查到 Master 节点进程的
pid，通过<code>kill -9</code>命令强制结束进程</p>
<p><img src="30.png"></p>
<p>结束进程后 <code>master:8989</code>
立刻变为不可访问状态，等待十几秒后查看
<code>slave1:8989</code>，发现状态已由 <strong>STANDBY</strong> 变成了
<strong>ALIVE</strong></p>
<p><img src="31.png"></p>
<h3 id="spark与hbase交互">Spark与HBase交互</h3>
<h4 id="配置hbase">配置HBase</h4>
<p>进入<a
target="_blank" rel="noopener" href="https://apache.org/dyn/closer.cgi">Apache官网</a>或直接进入<a
target="_blank" rel="noopener" href="https://apache.org/dyn/closer.cgi">清华源apache镜像</a>，下载hbase压缩包
<code>hbase-2.4.1-bin.tar.gz</code>；</p>
<p>解压缩到三台虚拟机 <code>/usr/local/</code> 路径下，默认文件夹名为
<code>hbase-2.4.1</code></p>
<p>配置 <code>etc/profile</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_HOME=/usr/local/hbase-2.4.1</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;HBASE_HOME&#125;</span>/bin:<span class="variable">$&#123;PATH&#125;</span></span><br></pre></td></tr></table></figure>
<p>进入 <code>hbase-2.4.1/conf</code> 路径下，配置
<code>hbase-env.sh</code> 文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/local/java/jdk1.8</span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>相同路径下配置 <code>regionserver</code>
文件，删除原有内容，添加三台主机名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
<p>相同路径下配置 <code>hbase-site.xml</code> 文件</p>
<p>删除原有的 <code>&lt;property&gt;</code> 元素块，添加如下配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HBase的Region的共享目录, 用于将HBase存入硬盘, 通常给出一个HDFS的目录, 用hdfs://$namenode:hdfsport(查看hadoop的core-site.xml配置), 如不设定, 则保存在本机的/tmp下 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span> hbase.rootdir <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- HMaster配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- HMaster配置web页面端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.info.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>16010<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- HBase的运行模式, false代表是单机运行模式, true代表是分布式模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置zookeeper的三台主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master,slave1,slave2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- zookeeper端口, 在zookeeper的conf/zoo.cfg文件中设定 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- zookeeper会话超时时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.session.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- zookeeper数据存储位置(dataDir) --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/zookeeper-3.6.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用hdfs支持 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.support.append<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定wal提供者，避免AsyncFSWAL初始化失败 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.wal.provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>filesystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在master中启动hdfs节点
<code>start-dfs.sh</code>，并在hdfs根目录中创建 <code>hbase</code>
文件夹</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">mkdir</span> /hbase</span><br></pre></td></tr></table></figure>
<h4 id="验证hbase">验证Hbase</h4>
<h5 id="启动">启动</h5>
<p>master启动dfs</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>三台机子均启动zookeeper服务器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure>
<p>在master中启动 Hbase Master</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure>
<p>打开浏览器，通过端口 <code>master:16010</code> 访问 HMaster</p>
<p><img src="33.png"></p>
<p>如果启动过程报错，或启动后HMaster进程自动消失了，可以查看
<code>hbase/logs</code> 路径下的日志文件
<code>hbase-node-master-master.log</code>，查找 failed 或 error
关键字定位错误，对症下药。</p>
<p>此外，hbase提供了交互式shell，通过 <code>hbase shell</code>
命令可启动</p>
<p><img src="34.png"></p>
<h5 id="关闭">关闭</h5>
<p>master中关闭 HMaster</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-hbase.sh</span><br></pre></td></tr></table></figure>
<p>关闭三台机子的 zookeeper 服务器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh stop</span><br></pre></td></tr></table></figure>
<p>关闭三台机子的 regionserver</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase-daemon.sh stop regionserver</span><br></pre></td></tr></table></figure>
<p>出现异常，需要单独关闭 HMaster 的命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase-daemon.sh stop master</span><br></pre></td></tr></table></figure>
<h4 id="创建hbase数据表">创建Hbase数据表</h4>
<p>注意启动顺序为</p>
<ul>
<li><p>三台 <code>zkServer.sh start</code></p></li>
<li><p>master <code>hadoop/sbin/start-dfs.sh</code></p></li>
<li><p>master <code>spark/sbin/start-all.sh</code></p></li>
<li><p>master <code>hbase/bin/start-hbase.sh</code></p></li>
<li><p>任意一台 <code>hbase shell</code></p></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">hbase:001:0&gt; create &#x27;student&#x27;,&#x27;info&#x27;</span><br><span class="line">Created table student</span><br><span class="line">Took 1.8511 seconds                                                                                     </span><br><span class="line">=&gt; Hbase::Table - student</span><br><span class="line">hbase:002:0&gt; put &#x27;student&#x27;,&#x27;1&#x27;,&#x27;info:name&#x27;,&#x27;frank&#x27;</span><br><span class="line">Took 0.2428 seconds                                                                                     </span><br><span class="line">hbase:003:0&gt; put &#x27;student&#x27;,&#x27;1&#x27;,&#x27;info:gender&#x27;,&#x27;male&#x27;</span><br><span class="line">Took 0.0124 seconds                                                                                     </span><br><span class="line">hbase:004:0&gt; put &#x27;student&#x27;,&#x27;1&#x27;,&#x27;info:age&#x27;,&#x27;22&#x27;</span><br><span class="line">Took 0.0240 seconds                                                                                     </span><br><span class="line">hbase:005:0&gt; put &#x27;student&#x27;,&#x27;2&#x27;,&#x27;info:name&#x27;,&#x27;lilly&#x27;</span><br><span class="line">Took 0.0169 seconds                                                                                     </span><br><span class="line">hbase:006:0&gt; put &#x27;student&#x27;,&#x27;2&#x27;,&#x27;info:gender&#x27;,&#x27;female&#x27;</span><br><span class="line">Took 0.0110 seconds                                                                                     </span><br><span class="line">hbase:007:0&gt; put &#x27;student&#x27;,&#x27;2&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;</span><br><span class="line">Took 0.0108 seconds                                                                                     </span><br><span class="line">hbase:008:0&gt; list &#x27;student&#x27;</span><br><span class="line">TABLE                                                                                                   </span><br><span class="line">student                                                                                                 </span><br><span class="line">1 row(s)</span><br><span class="line">Took 0.0564 seconds                                                                                     </span><br><span class="line">=&gt; [&quot;student&quot;]</span><br><span class="line">hbase:009:0&gt; describe &#x27;student&#x27;</span><br><span class="line">Table student is ENABLED                                                                                </span><br><span class="line">student                                                                                                 </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                                                             </span><br><span class="line">&#123;NAME =&gt; &#x27;info&#x27;, BLOOMFILTER =&gt; &#x27;ROW&#x27;, IN_MEMORY =&gt; &#x27;false&#x27;, VERSIONS =&gt; &#x27;1&#x27;, KEEP_DELETED_CELLS =&gt; &#x27;FAL</span><br><span class="line">SE&#x27;, DATA_BLOCK_ENCODING =&gt; &#x27;NONE&#x27;, COMPRESSION =&gt; &#x27;NONE&#x27;, TTL =&gt; &#x27;FOREVER&#x27;, MIN_VERSIONS =&gt; &#x27;0&#x27;, BLOCKC</span><br><span class="line">ACHE =&gt; &#x27;true&#x27;, BLOCKSIZE =&gt; &#x27;65536&#x27;, REPLICATION_SCOPE =&gt; &#x27;0&#x27;&#125;                                         </span><br><span class="line"></span><br><span class="line">1 row(s)</span><br><span class="line">Quota is disabled</span><br><span class="line">Took 0.1049 seconds</span><br></pre></td></tr></table></figure>
<h4 id="拷贝jar包">拷贝jar包</h4>
<p>将 <code>hbase/libs</code> 路径下以"hbase"开头的所有
<code>.jar</code> 包拷贝到 <code>spark/jars</code> 路径下</p>
<h4 id="测试程序">测试程序</h4>
<p>启动spark-shell，测试程序是否可用，程序来源见 <a
target="_blank" rel="noopener" href="https://blog.csdn.net/lz6363/article/details/109016389?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">Spark使用newAPIHadoopRDD方式读取HBase表</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;<span class="type">CellUtil</span>, <span class="type">HBaseConfiguration</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Result</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.<span class="type">KryoSerializer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;ReadEventLogsSparkDemo&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">            .set(<span class="string">&quot;spark.serializer&quot;</span>, classOf[<span class="type">KryoSerializer</span>].getName)</span><br><span class="line">            .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">ImmutableBytesWritable</span>],classOf[<span class="type">Result</span>]))</span><br><span class="line">        <span class="comment">// 创建SparkContext 实例对象</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">SparkContext</span>.getOrCreate(sparkConf)</span><br><span class="line">        <span class="comment">// 设置日志级别  Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN</span></span><br><span class="line">        sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line">        <span class="comment">// a. 读取配置信息</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;192.168.56.104&quot;</span>)</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line">        <span class="comment">// b. 设置从HBase那张表读取数据</span></span><br><span class="line">        conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>,<span class="string">&quot;student&quot;</span>)</span><br><span class="line">        <span class="comment">// c. 调用SparkContext中newAPIHadoopRDD读取表中的数据</span></span><br><span class="line">        <span class="keyword">val</span> resultRDD = sc.newAPIHadoopRDD(</span><br><span class="line">            conf,</span><br><span class="line">            classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">            classOf[<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">            classOf[<span class="type">Result</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// 测试获取的数据</span></span><br><span class="line">        println(<span class="string">s&quot;RDD count = <span class="subst">$&#123;resultRDD.count()&#125;</span>&quot;</span>)</span><br><span class="line">        resultRDD.take(<span class="number">2</span>).foreach&#123;</span><br><span class="line">            <span class="keyword">case</span> (key, result) =&gt;</span><br><span class="line">                println(<span class="string">s&quot;rowKey = <span class="subst">$&#123;Bytes.toString(result.getRow)&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">for</span> (cell &lt;- result.rawCells()) &#123;</span><br><span class="line">                    <span class="keyword">val</span> cf = <span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneFamily(cell))</span><br><span class="line">                    <span class="keyword">val</span> colum = <span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneQualifier(cell))</span><br><span class="line">                    <span class="keyword">val</span> value = <span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneValue(cell))</span><br><span class="line">                    println(<span class="string">s&quot;\t<span class="subst">$cf</span>:<span class="subst">$colum</span> = <span class="subst">$value</span>&quot;</span>)</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭资源</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将import与main函数的内容复制到spark-shell中执行，确保程序没有问题后进行下一步</p>
<h4 id="打包jar">打包jar</h4>
<p>这里使用的环境与工具是 <a
target="_blank" rel="noopener" href="https://www.jetbrains.com/idea/">Intellij IDEA</a> + <a
target="_blank" rel="noopener" href="https://maven.apache.org/">Maven</a></p>
<p>创建一个Maven项目，在<code>File</code>-&gt;<code>setting</code>中设置好maven包路径以及配置文件路径，通过<code>File</code>-&gt;<code>Project Structure</code>-&gt;<code>Global Libraries</code>添加scala-sdk，并右键项目名，选择<code>Add Framework Support</code>添加scala框架支持；</p>
<p>配置 <strong>pom.xml</strong> 依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>3.3.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>3.0.1<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hbase.version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">hbase.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Spark依赖 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hadoop依赖 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hbase依赖 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">type</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-mapreduce<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>点击
<code>File</code>-&gt;<code>Project Structure</code>-&gt;<code>Artifacts</code>-&gt;<code>+</code>-&gt;<code>JAR</code>-&gt;<code>From modules with dependencies</code>，浏览选择主类</p>
<p><img src="43.png"></p>
<p>点击 <code>OK</code>，删除Output
Layout左侧子栏目中除了目标jar之外的其他所有jar依赖，单击目标jar，下方可确认
<code>Main class</code> 的名称，也就是在执行 <code>spark-submit</code>
时 <code>--class</code> 指定的参数名</p>
<p><img src="44.png"></p>
<p>确认后点击
<code>build</code>-&gt;<code>build Artifacts</code>-&gt;<code>build</code>
即可开始编译；</p>
<p>编译完成后即在项目的 <code>/out</code> 路径下生成jar包</p>
<p><img src="45.png"></p>
<p>将jar包复制到共享文件夹中，或直接发送到任意一台虚拟机上，提交应用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class Test \</span><br><span class="line">--master spark://master:7077 \</span><br><span class="line">/mnt/share/testProj.jar 10</span><br></pre></td></tr></table></figure>
<p><img src="46.png"></p>
<h2 id="遇到的问题与解决方案">遇到的问题与解决方案</h2>
<p><strong>注意点一：</strong>hadoop要求主机名<strong>不带</strong>下划线<code>_</code>；</p>
<p><strong>注意点二：</strong>每台运行hadoop的主机的<strong>用户名</strong>需要相同；</p>
<p><strong>注意点三：</strong>使用 <code>chown -R</code>
将hadoop、spark等相关文件夹所有者变更为 <code>node</code>，否则会有很多
<code>permission denied</code> 的错误</p>
<p><strong>问题一：</strong>启动 <code>hdfs namenode -format</code>
报错，内容大致是无法创建或写入logs文件夹；</p>
<ul>
<li>原因：所在用户（node）无法对hadoop路径下的文件进行写操作；</li>
<li>解决方案：<code>chown node /usr/local/hadoop-3.3.0</code>；</li>
</ul>
<h3 id="向hdfs执行put命令时报错">向HDFS执行put命令时报错：</h3>
<p><img src="13.png" style=zoom:100%></p>
<ul>
<li>原因：格式化多次导致 DataNode 运行异常；</li>
<li>解决方案：<a
target="_blank" rel="noopener" href="https://www.it610.com/article/1305328093019279360.htm">参考博客</a>，按以下步骤进行：
<ul>
<li><code>stop-all.sh</code> 关闭所有节点；</li>
<li>删除每台服务器 <code>/usr/local/hadoop-3.3.0/data/tmp/dfs/</code>
路径下所有文件夹中的 <code>current</code>
子文件夹，NameNode节点所在服务器路径下有两个文件夹，分别是
<code>name</code> 和
<code>data</code>，yarn-resourceManager节点所在服务器路径下只有
<code>data</code> 文件夹，辅助NameNode节点所在服务器路径下有
<code>namesecondary</code> 和 <code>data</code> 两个文件夹；</li>
<li>Master服务器中重新格式化NameNode
<code>hadoop namenode -format</code>；</li>
<li>分别在Master和作为Yarn.resourceManager的服务器上运行
<code>start-dfs.sh</code> 和 <code>start-yarn.sh</code>；</li>
<li>使用 <code>jps</code> 查看 DataNode 是否正常运行；</li>
<li>可使用 <code>hdfs dfsadmin -report</code>
查看服务器集群的状态；</li>
</ul></li>
</ul>
<p><img src="14.png" alt="三台服务器的dfs路径" style=zoom:100%></p>
<p><img src="15.png" alt="DataNode运行正常" style=zoom:100%></p>
<p><img src="16.png" alt="测试成功" style=zoom:100%></p>
<h3
id="vscode插件remote-ssh使用方式">VScode插件Remote-SSH使用方式：</h3>
<ul>
<li><p>F1，输入ssh可以查看补全选项；</p></li>
<li><p>选configuration可以编辑连接对象的默认配置文件，在home和etc路径下各有一个，改一个就行；</p></li>
<li><p>选Connect current window to
host可以连接指定配置文件中的指定host；</p>
<ul>
<li>此步骤弹窗报错：<code>could not establish connection to XXX</code>，见下方情况一；</li>
</ul></li>
<li><p>上一步通过，但下一步输入密码后显示 <code>permission denied</code>
见下方情况二；</p></li>
<li><p>情况一：不论使用 <code>ssh</code> 登录什么账户，都显示
<code>could not establish connection to XXX</code>；</p>
<ul>
<li>原因：服务器端（被访问端）未安装ssh；</li>
<li>解决方案：<code>sudo apt install openssh-server</code>；
<ul>
<li>重启 ssh 服务：<code>service ssh restart</code>；</li>
<li>查看 ssh 服务状态：<code>ps -e | grep ssh</code> 或
<code>service ssh status</code>；</li>
</ul></li>
</ul></li>
<li><p>情况二：直接在终端使用<code>ssh root@ip</code> 连接出现
<code>permission denied</code> 但使用普通用户可以登录；</p>
<ul>
<li>原因一：root用户密码为系统给的默认值，没有修改过，自然每次密码错误都是permission
denied；
<ul>
<li><code>su</code>与<code>sudo</code>的区别：<code>su</code>是切换用户，不加参数默认切换到root用户，需要输入root用户的密码才能切换，而<code>sudo</code>指的是可在任意用户下直接使用超级用户权限(root用户的权限)执行命令，这只是一种授权指令，确保执行命令的用户知道自己将用root权限，因此需要的是执行命令的用户的密码，而非root用户的密码；</li>
<li>解决方案：<code>sudo passwd root</code>，更改root用户密码，即可用root用户登录；</li>
</ul></li>
<li>原因二：服务器端设定了不可用root权限远程登录；
<ul>
<li>解决方案：<code>sudo vim /etc/ssh/sshd_config</code>，将
<code>PermitRootLogin</code> 的值设为 yes，重启ssh服务；</li>
</ul></li>
</ul></li>
</ul>
<h3 id="ubuntu下xshell的替代软件">Ubuntu下XShell的替代软件</h3>
<p>XShell在linux下无法使用，我们可以使用开源软件<a
target="_blank" rel="noopener" href="https://github.com/asbru-cm/asbru-cm">asbru-cm</a>达到相似的效果。</p>
<p>asbru-c官方文档：https://docs.asbru-cm.net/</p>
<p>安装asbru-cm</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -1sLf <span class="string">&#x27;https://dl.cloudsmith.io/public/asbru-cm/release/cfg/setup/bash.deb.sh&#x27;</span> | sudo -E bash</span><br><span class="line">sudo apt-get install asbru-cm</span><br></pre></td></tr></table></figure>
<p>打开软件，点击左上角图标 <code>New GROUP</code>，创建服务器组
<code>Hadoop</code>；</p>
<p>在该组下点击左上角第二个图标
<code>New CONNECTION</code>，连接名称填写
<code>Master</code>，进入ssh配置，其中 <code>Host</code>
填写ip，<code>port</code>默认为22，<code>Authentication</code>
一栏填入用户名和密码。；</p>
<p><img src="19.png" style=zoom:100%></p>
<p>同样的方式创建与 <code>Slave1</code> 和 <code>Slave2</code>
的连接，创建完毕后，选择 <code>Hadoop</code> 组，点击下方的
<code>connect</code> 按钮，即可同时建立与三台服务器的ssh连接；</p>
<p><img src="20.png" style=zoom:100%></p>
<p>点击左下角 <code>Cluster</code>，右侧栏目中点击
<code>添加</code>，创建一个cluster，并将左侧 <code>Hadoop</code>
组内三台服务器都添加到该cluster内；</p>
<p><img src="21.png" style=zoom:100%></p>
<p>点击左下角
<code>PCC</code>，如果不是从cluster启动连接的话，在PCC界面无法选择cluster，也无需勾选<code>Send to ALL Terminals</code>，默认就是发送到所有终端，在
<code>Type commmands here</code>
一栏可直接输入命令同时与三个终端进行交互，注意后面的
<code>Send on &lt;INTRO&gt;</code>
不勾选时，输入的内容是直接反馈到右侧终端界面的，不会显示在文本框内；</p>
<p><img src="22.png" style=zoom:100%></p>
<h3 id="个人犯蠢教训">个人犯蠢教训</h3>
<p>进行 <code>spark-submit</code> 提交应用进行一半时不小心
<code>CTRL+C</code> 强制中断了，之后再也提交不成功，报错如下：</p>
<p><img src="32.png"></p>
<p>我傻了，<code>--class</code> 入口类名应该是
<code>org.apache.spark.examples.SparkPi</code>，我后来的提交都是手打的，少写了个
<code>.spark</code>，导致无法找到并加载这个类，跟什么强制中断没有用半毛钱关系，自始至终我都丝毫没怀疑过这个能敲错，真是丢人现眼长记性了。</p>
<h3
id="高可用spark两台master端口全部显示standby">高可用Spark两台Master端口全部显示STANDBY</h3>
<p>配置完Zookeeper后发现master和slave1在8989端口上均处于
<strong>STANDBY</strong> 状态</p>
<p>起初以为是配置文件的问题，但不论是改 <code>zoo.cfg</code> 还是改
<code>spark-env.sh</code>
都没用，最后发现是因为我只在master上启动了zkServer，而slave上没有启动，具体原因我暂时也不明白，后来三台都启动zk就好了;</p>
<h3 id="hbase-shell中执行create报错">hbase shell中执行create报错</h3>
<p>报错关键句：<code>ERROR: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet</code>；</p>
<p>在网上找到的大部分解决办法都是关闭hadoop的安全模式，但我通过
<code>hdfs dfsadmin -safemode get</code>
命令查看到安全模式本身就是关闭的，再关闭也无济于事；</p>
<p><img src="35.png"></p>
<p>在尝试网上各种办法期间还碰到了启动hbase后HMaster进程自动关闭的问题，我没有照网上大部分教程说的启动
<code>zkCli.sh</code>，删除hbase节点再重启zk，而是直接执行
<code>hdfs namenode -format</code>
把namenode格式化了，又导致datanode出问题参考问题二的解决方式重复了一遍，然后启动dfs，在根目录重新创建spark历史服务需要的
<code>/logs</code> 目录，以及hbase需要的
<code>/hbase</code>目录，绕了一大圈重新恢复HMaster之后仍然没有解决hbase
shell的问题；</p>
<p><img src="36.png" alt="HMaster崩溃在日志里记录的报错"></p>
<p>解决HMaster问题仍然报错后，重新查看了 <code>/hbase/logs</code>
路径下的 <code>hbase-node-master-master.log</code>
日志文件，发现记录了一个关键错误：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2021-01-31 05:00:23,966 WARN  [RS-EventLoopGroup-1-1] concurrent.DefaultPromise: An exception was thrown by org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper$4.operationComplete()</span><br><span class="line">java.lang.IllegalArgumentException: object is not an instance of declaring class ......</span><br></pre></td></tr></table></figure>
<p><img src="37.png"></p>
<p>照这个错在网上又是一顿查，最后在 <a
target="_blank" rel="noopener" href="https://www.yuque.com/apachecn/hbase-doc-zh/docs_20">ApacheCN-Hbase社区文档</a>
第136条RegionServer中找到了问题所在：</p>
<blockquote>
<p>我们将尝试将 AsyncFSWAL 用于
HBase-2.x，因为它具有更好的性能，同时消耗更少的资源。但 AsyncFSWAL
的问题在于它侵入了 DFSClient 实现的内部，因此在升级 hadoop
时很容易被破解，即使是简单的补丁发布也是如此。如果你没有指定 wal
提供者，如果我们无法初始化 AsyncFSWAL ，我们将尝试回退到旧的 FSHLog
，但它可能并不总是有效。失败将显示在这样的日志中：</p>
</blockquote>
<p>官方指定需要在 <code>hbase-site.xml</code>
文件中配置一个property块，内容如下</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.wal.provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>filesystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>问题解决 T_T</p>
<p><img src="38.png"></p>
<h3 id="spark启动start-all.sh报错">Spark启动start-all.sh报错</h3>
<p>spark启动 <code>sbin/start-all.sh</code>
异常，终端内显示的报错信息为</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">failed to launch: <span class="built_in">nice</span> -n 0 /usr/local/spark-3.0.1/bin/spark-class org.apache.spark.deploy.master.Master --host master --port 7077 --webui-port 8080</span><br></pre></td></tr></table></figure>
<p><img src="39.png"></p>
<p>master日志文件记录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">21/02/01 01:05:38 ERROR SparkUncaughtExceptionHandler: Another uncaught exception in thread Thread[main,5,main], process halted.</span><br></pre></td></tr></table></figure>
<p><img src="40.png"></p>
<p>原因：在解决之前的问题时无意注释掉了 <code>spark-env.sh</code>
中的一行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-3.3.0/bin/hadoop classpath)</span><br></pre></td></tr></table></figure>
<p>取消注释或重新加上这行就解决了</p>
<h3
id="spark-shell中执行import引入包报错">spark-shell中执行import引入包报错</h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="type">ERROR</span>: <span class="class"><span class="keyword">object</span> <span class="title">hbase</span> <span class="title">is</span> <span class="title">not</span> <span class="title">a</span> <span class="title">member</span> <span class="title">of</span> <span class="title">package</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">hadoop</span></span></span><br></pre></td></tr></table></figure>
<p>原因：从 <code>hbase/libs</code> 中将以"hbase"开头的
<code>.jar</code> 包拷贝到 <code>spark/jars</code>
路径下时，一开始为了将拷贝进来的包隔离开，多创建了一级文件夹
<code>spark/jars/hbase</code>，导致在spark-shell中 <code>import</code>
找不到 <code>hbase-common</code> 的包。</p>
<p>解决方案：拷贝时直接贴到 <code>spark/jars</code> 路径中即可。</p>
<p>另外有一点，当时参考网上教程，说除了以"hbase"开头的包外，还需要拷贝两个包，分别是：</p>
<ul>
<li>guava-11.0.2.jar</li>
<li>protobuf-java-2.5.0.jar</li>
</ul>
<p>我在hbase路径中没找到这两个包，还特地去网上看了五花八门的解决方案，有让手动下载的，有让重装的，最后我非常无语地在
<code>spark/jars</code> 里发现这俩包本来就在spark里有。</p>
<p>当无知的脑子碰上误导人的网上教程，得走太多弯路。</p>
<h3 id="spark-submit提交jar包报错">spark-submit提交jar包报错</h3>
<p>报错内容：<code>Error: Failed to load class Test.</code></p>
<p><img src="41.png"></p>
<p>解决办法：使用idea打包时需要将 <code>Output Layout</code>
一栏中除了目标jar之外的其他jar全部删除，只留complie
output文件（META-INF文件夹是否保留不影响），如图所示</p>
<p><img src="42.png"></p>
<p>另外，执行spark-submit时需要确保 <code>--class</code>
的参数与打包时设置的<code>Main Class</code>一致</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class Test --master spark://master:7077 /mnt/share/testProj.jar 10</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/archives/3d05c460/" rel="prev" title="主机ubuntu与Vbox虚拟机ubuntu-server实现文件共享">
      <i class="fa fa-chevron-left"></i> 主机ubuntu与Vbox虚拟机ubuntu-server实现文件共享
    </a></div>
      <div class="post-nav-item">
    <a href="/archives/5089e8f6/" rel="next" title="Windows10 + VS Code 配置 C++ OpenCV-4.5.5 + Opencv_Contrib-4.5.5 开发环境">
      Windows10 + VS Code 配置 C++ OpenCV-4.5.5 + Opencv_Contrib-4.5.5 开发环境 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8virtualbox%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8Eubuntu-server%E7%9A%84%E4%B8%89%E6%9C%BAhadoop%E9%9B%86%E7%BE%A4"><span class="nav-number">1.</span> <span class="nav-text">使用VirtualBox搭建基于Ubuntu-Server的三机Hadoop集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92"><span class="nav-number">1.1.</span> <span class="nav-text">集群规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAmaster%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83"><span class="nav-number">1.2.</span> <span class="nav-text">创建Master虚拟机并配置环境</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="nav-number">1.2.1.</span> <span class="nav-text">创建虚拟机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85sshjdk%E4%B8%8Ehadoop"><span class="nav-number">1.2.2.</span> <span class="nav-text">安装SSH、JDK与Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AEssh"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">安装并配置SSH</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85jdk%E4%B8%8Ehadoop"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">安装JDK与Hadoop</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEhadoop"><span class="nav-number">1.2.3.</span> <span class="nav-text">配置Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#core-site.xml"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">core-site.xml</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hdfs-site.xml"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">hdfs-site.xml</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yarn-site.xml"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">yarn-site.xml</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mapred-site.xml"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">mapred-site.xml</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hadoop-env.sh-%E4%B8%8E-yarn-env.sh"><span class="nav-number">1.2.3.5.</span> <span class="nav-text">hadoop-env.sh 与 yarn-env.sh</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#workers"><span class="nav-number">1.2.3.6.</span> <span class="nav-text">workers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9hosts%E6%96%87%E4%BB%B6"><span class="nav-number">1.2.3.7.</span> <span class="nav-text">修改hosts文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E8%BF%87master%E5%89%AF%E6%9C%AC%E5%88%9B%E5%BB%BAslave%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="nav-number">1.3.</span> <span class="nav-text">通过Master副本创建Slave虚拟机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%85%B3%E9%97%ADhadoop"><span class="nav-number">1.4.</span> <span class="nav-text">启动与关闭Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8hadoop"><span class="nav-number">1.4.1.</span> <span class="nav-text">启动Hadoop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87ip%E8%AE%BF%E9%97%AEhadoop%E9%9B%86%E7%BE%A4"><span class="nav-number">1.4.2.</span> <span class="nav-text">通过ip访问Hadoop集群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%97%ADhadoop"><span class="nav-number">1.4.3.</span> <span class="nav-text">关闭Hadoop</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEspark3.0%E7%8E%AF%E5%A2%83"><span class="nav-number">2.</span> <span class="nav-text">配置Spark3.0环境</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BDspark%E5%8E%8B%E7%BC%A9%E5%8C%85"><span class="nav-number">2.1.</span> <span class="nav-text">下载Spark压缩包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEspark-env.sh"><span class="nav-number">2.2.</span> <span class="nav-text">配置spark-env.sh</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEslaves%E6%96%87%E4%BB%B6"><span class="nav-number">2.3.</span> <span class="nav-text">配置slaves文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="nav-number">2.4.</span> <span class="nav-text">配置环境变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81%E6%9C%AC%E5%9C%B0spark"><span class="nav-number">2.5.</span> <span class="nav-text">验证本地Spark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8spark-shell"><span class="nav-number">2.6.</span> <span class="nav-text">使用Spark Shell</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81%E5%88%86%E5%B8%83%E5%BC%8Fspark"><span class="nav-number">2.7.</span> <span class="nav-text">验证分布式Spark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEspark%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1"><span class="nav-number">2.8.</span> <span class="nav-text">配置Spark历史服务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E9%80%89%E9%83%A8%E5%88%86"><span class="nav-number">3.</span> <span class="nav-text">可选部分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEzookeeper%E4%B8%8Espark%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4"><span class="nav-number">3.1.</span> <span class="nav-text">配置Zookeeper与Spark高可用集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark%E4%B8%8Ehbase%E4%BA%A4%E4%BA%92"><span class="nav-number">3.2.</span> <span class="nav-text">Spark与HBase交互</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEhbase"><span class="nav-number">3.2.1.</span> <span class="nav-text">配置HBase</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81hbase"><span class="nav-number">3.2.2.</span> <span class="nav-text">验证Hbase</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B3%E9%97%AD"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">关闭</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAhbase%E6%95%B0%E6%8D%AE%E8%A1%A8"><span class="nav-number">3.2.3.</span> <span class="nav-text">创建Hbase数据表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8B%B7%E8%B4%9Djar%E5%8C%85"><span class="nav-number">3.2.4.</span> <span class="nav-text">拷贝jar包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F"><span class="nav-number">3.2.5.</span> <span class="nav-text">测试程序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%93%E5%8C%85jar"><span class="nav-number">3.2.6.</span> <span class="nav-text">打包jar</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">4.</span> <span class="nav-text">遇到的问题与解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91hdfs%E6%89%A7%E8%A1%8Cput%E5%91%BD%E4%BB%A4%E6%97%B6%E6%8A%A5%E9%94%99"><span class="nav-number">4.1.</span> <span class="nav-text">向HDFS执行put命令时报错：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vscode%E6%8F%92%E4%BB%B6remote-ssh%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="nav-number">4.2.</span> <span class="nav-text">VScode插件Remote-SSH使用方式：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ubuntu%E4%B8%8Bxshell%E7%9A%84%E6%9B%BF%E4%BB%A3%E8%BD%AF%E4%BB%B6"><span class="nav-number">4.3.</span> <span class="nav-text">Ubuntu下XShell的替代软件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E7%8A%AF%E8%A0%A2%E6%95%99%E8%AE%AD"><span class="nav-number">4.4.</span> <span class="nav-text">个人犯蠢教训</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8spark%E4%B8%A4%E5%8F%B0master%E7%AB%AF%E5%8F%A3%E5%85%A8%E9%83%A8%E6%98%BE%E7%A4%BAstandby"><span class="nav-number">4.5.</span> <span class="nav-text">高可用Spark两台Master端口全部显示STANDBY</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hbase-shell%E4%B8%AD%E6%89%A7%E8%A1%8Ccreate%E6%8A%A5%E9%94%99"><span class="nav-number">4.6.</span> <span class="nav-text">hbase shell中执行create报错</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark%E5%90%AF%E5%8A%A8start-all.sh%E6%8A%A5%E9%94%99"><span class="nav-number">4.7.</span> <span class="nav-text">Spark启动start-all.sh报错</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-shell%E4%B8%AD%E6%89%A7%E8%A1%8Cimport%E5%BC%95%E5%85%A5%E5%8C%85%E6%8A%A5%E9%94%99"><span class="nav-number">4.8.</span> <span class="nav-text">spark-shell中执行import引入包报错</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-submit%E6%8F%90%E4%BA%A4jar%E5%8C%85%E6%8A%A5%E9%94%99"><span class="nav-number">4.9.</span> <span class="nav-text">spark-submit提交jar包报错</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Lightshaker</p>
  <div class="site-description" itemprop="description">It is our choices that show what we truly are, far more than our abilities.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lightshaker</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'j3EqSOqSYjPS414YtmiadN5W-gzGzoHsz',
      appKey     : '429Gb8wTQv1P09Fjbeyd9k5w',
      placeholder: "Hello",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
